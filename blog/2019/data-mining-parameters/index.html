<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Data Mining Parameters | Sudip Bhujel</title> <meta name="author" content="Sudip Bhujel"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="sudip-bhujel"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.ico?a056b6674bc802f0293330fba6e57604"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudipbhujel.github.io/blog/2019/data-mining-parameters/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sudip </span>Bhujel</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Data Mining Parameters</h1> <p class="post-meta">December 28, 2019• Sudip Bhujel</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/datamining"> <i class="fas fa-hashtag fa-sm"></i> datamining</a>   <a href="/blog/tag/eda"> <i class="fas fa-hashtag fa-sm"></i> EDA</a>     ·   <a href="/blog/category/journal"> <i class="fas fa-tag fa-sm"></i> journal</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Datamining covers everything that are related with the data from collection of raw data to EDA and preparation of input to AI algorithm. We have lots of parameters for describing the data. Some of them we are going to discuss are Impurity index, Central of tendency, Eigenvalue/ Eigenvector, PCA in Classification. <br> <strong> Abstract: <i> The impurities measurement parameter of dataset like Entropy, Gini, Classification Error aims to find the error while classifying the labels. The attribute with less value of impurity will be chosen out of attribute contenders. The measure of central tendency like mean, median, quartiles, etc. along with boxplot gives the idea about the distribution of data and outliers which leads then how to treat the data to get the most information out of it. The features/ attributes are important parameters for any machine learning algorithm, large-sized attributes result in a more accurate prediction which means that the model has high accuracy. The computational cost for a model with a large number of attributes is generally high. The best model is that which takes as least attributes as possible without losing the information and has reasonable accuracy. Principal Component Analysis (PCA) is a feature extraction method that uses orthogonal linear projections to capture the underlying variance of the data. It reduces the number of least wanted features for prediction without losing the overall information of data. </i></strong></p> <h2 id="entropy">Entropy</h2> <p>Entropy is a measure of impurity, disorder or uncertainty in a bunch of examples i.e. it is an indicator of how messy our data is. In Decision Trees, the goal is to tidy the data. Entropy controls how a Decision Tree decides to split the data. It affects how a Decision Tree draws its boundaries so that the outcomes from the algorithm will have purely classified objects.</p> \[E(x) = \sum_{x\epsilon X} p(x)log_2 p(x)\] <p>Where, S = The current dataset for which entropy is being calculated <br> X = Set of classes in S<br> p(x) = The probability of each set S</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/impurity-vs-probability-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/impurity-vs-probability-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/impurity-vs-probability-1400.webp"></source> <img src="/assets/img/impurity-vs-probability.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="color:grey; text-align:center; font-style:italic"> Impurity Index versus Probability, Impurity Indices are Entropy, Gini, and Classification Error</p> <h2 id="gini">Gini</h2> <p>Impurity measures such as entropy and Gini index tend to favor attributes that have a large number of distinct values . If we consider the same example as in entropy, the gini index is computed using the following equation:</p> \[G(S) = 1-\sum_{x\epsilon X} |p(x)|^2\] <p>Where, S = The current dataset for which entropy is being calculated <br> X = Set of classes in S <br> p(x) = The probability of each set S</p> <h2 id="classification-error">Classification Error</h2> <p>Classification error is a measure of impurity at a node and defined for classification error at a node t as,</p> \[Error(t) = 1 − maxP(i|t)\] <p>The classification error made by node ranges minimum 0 when all records belong to one class to maximum \((1 − 1/n_c )\) when records are equally distributed among all classes.</p> <h2 id="covariance-matrix">Covariance Matrix</h2> <p>Variance measures the variation of a single random variable (like the height of a person in a population), whereas covariance is a measure of how much two random variables vary together (like the height of a person and the weight of a person in a population). The covariance matrix can be calculated using covariance, which is a square matrix given by C I,j = σ(x i , x j ) where C ∈ R d xd and d describe dimension or number of random variables of the data (e.g. the number of features like height, width, weight, etc.). The calculation for the covariance matrix can be also expressed as:</p> \[C = \frac{1}{n-1} \sum_{i=1} ^n (X_i-\overline{X} )(X_i-\overline{X} )^T\] <p>The covariance matrix for two dimensions is given by,</p> \[\begin{pmatrix} \sigma(x,x) &amp; \sigma(x,y) \\\ \sigma(y,x) &amp; \sigma(y,y) \end{pmatrix}\] <p>The covariance matrix is symmetric since \(\sigma(x_i, x_j) = \sigma(x_j, x_i)\).</p> <h2 id="eigenvalue-and-eigenvector">Eigenvalue and Eigenvector</h2> <p>In linear algebra, an eigenvector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue is the factor by which the eigenvector is scaled. For linear equations:</p> \[Av = λv\] <p>In this equation A is an n-by-n matrix, v is a non-zero n-by-1 vector and \(\lambda\) is a scalar (which may be either real or complex). Any value of \(\lambda\) for which this equation has a solution is known as eigenvalue of the matrix A. It is sometimes also called the characteristics value. The vector, v, which corresponds to this value is called an eigenvector. The eigen problem can be written as</p> \[A. v − \lambda. v = 0 \\ A. v − \lambda. I. v = 0 \\ (A − \lambda. I). v = 0\] <p>If v is non-zero, this equation will only have a solution if \(|A − \lambda. I| = 0\) This equation is called the characteristic equation of A, and is an nth order polynomial in \(\lambda\) with n roots. These roots are called the eigenvalues of A. We will only deal with the case of n distinct roots, though they may be repeated. For each eigenvalue, there will be an eigenvector for which the eigenvalue equation is true.</p> <h2 id="distances">Distances</h2> <p><strong>Euclidean distance</strong> is a measure of the distance between two points in Euclidean space. Mathematically,</p> \[dist = \sqrt{\sum_{k=1}^n (p_k - q_k)^2}\] <p>Where n is the number of dimensions (attributes) and \(p_k\) and \(q_k\) are, respectively, the \(k^th\) attributes (components) or data objects p and q. <strong>Minkowski Distance </strong> is a generalization of Euclidean distance and given as,</p> \[dist = \left(\sum_{k=1}^n |p_k - q_k|^r \right)^{\frac{1}{r}}\] <p>Where r is a parameter, n is the number of dimensions (attributes) and \(p_k\) and \(q_k\) are, respectively, the k th attributes (components) or data objects p and q.</p> <ul> <li>r = 1, it becomes Manhattan distance.</li> <li>r = 2, it becomes Euclidean distance.</li> <li>\(r \to \infty\), it becomes supremum distance.</li> </ul> <h2 id="similarity">Similarity</h2> <p>The similarity is the measure of how much alike two data objects are. The similarity in a data mining context is usually described as a distance with dimensions representing features of the objects. A small distance indicating a high degree of similarity and a large distance indicating a low degree of similarity. The similarity is subjective and is highly dependent on the domain and application.<br> <strong>Cosine Similarity</strong> of two document vectors is given as,</p> \[cos(d_1, d_2) = \frac{d_1 . d_2}{||d_1||.||d_2||}\] <p>Where ||d|| is the length of vector d.<br> Cosine similarity is for comparing two real-valued vectors, but <strong>Jaccard similarity</strong> is for comparing two binary vectors (sets). Mathematically,</p> \[J_g (a,b) = frac{sum_i min(a_i, b_i)}{sum_i max(a_i, b_i)}\] <p>For example, \(t_1 = (1, 1,0,1), t_2 = (2,0,1,1)\), the generalized Jaccard similarity index can be computed as follows:</p> \[J(t_1, t_2) = \frac{1+0+0+1}{2+1+1+1} = 0.4\] <h2 id="pca">PCA</h2> <p>Principal Component Analysis (PCA) is a feature extraction method that uses orthogonal linear projections to capture the underlying variance of the data. The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. It reduces the dimension of the data with the aim of retaining as much information as possible. In other words, this method combines highly correlated variables to form a smaller number of an artificial set of variables which is called “principal components” that account for the most variance in the data.</p> <h2 id="conclusion">CONCLUSION</h2> <p>The measure of central of tendency, similarity, etc. are the part of Exploratory Data Analysis (EDA). The EDA itself doesn’t give the model for prediction but extremely useful for getting the sense of information from data. This gives an idea about how to get started with the data. Impurity indices like Entropy, Gini, and Classification Error in the classification helps examine how classification algorithm struggles to classify the items based on their attributes. The impurity index helps find the depth of the decision tree algorithm.</p> <h2 id="references">References</h2> <ul> <li><a href="https://www.amazon.com/Introduction-Mining-Whats-Computer-Science/dp/0133128903/ref=sr_1_5?keywords=data+mining&amp;qid=1577535801&amp;sr=8-5" rel="external nofollow noopener" target="_blank">P. Tan, M. Steinbach, V. Kumar and A. Karpatne, Introduction to Data Mining, Global Edition. Harlow, United Kingdom: Pearson Education Limited, 2019.</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/knn-nb-classifier/">k-Nearest Neighbors classifier, Naïve Bayes classifier in Data Mining </a> </li> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Sudip Bhujel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>