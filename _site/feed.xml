<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://sudipbhujel.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sudipbhujel.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-19T10:51:21+00:00</updated><id>https://sudipbhujel.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">k-Nearest Neighbors classifier, Naïve Bayes classifier in Data Mining</title><link href="https://sudipbhujel.github.io/blog/2020/knn-nb-classifier/" rel="alternate" type="text/html" title="k-Nearest Neighbors classifier, Naïve Bayes classifier in Data Mining"/><published>2020-01-07T00:00:00+00:00</published><updated>2020-01-07T00:00:00+00:00</updated><id>https://sudipbhujel.github.io/blog/2020/knn-nb-classifier</id><content type="html" xml:base="https://sudipbhujel.github.io/blog/2020/knn-nb-classifier/"><![CDATA[<h2 id="i-introduction">I. Introduction</h2> <p>Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. The machine learning is classified into different categories viz. supervised machine learning, unsupervised learning, semi-supervised learning, and reinforcement machine learning. The supervised learning algorithm takes features as input, maps to a mapping function and approximates a result. The goal is to approximate the mapping function so well that when it gets new input that it can predict the output variables for that data. The supervised learning algorithm aims to find the pattern of the features to a particular result.</p> <p>Classification, which is the task of assigning objects to one of several predefined categories, is a pervasive problem that encompasses many diverse applications. Examples include detecting spam email messages based upon the message header and content, categorizing cells as malignant or benign based upon the results of MRI scans, and classifying galaxies based upon their shapes.</p> <p>The classification problems like email is spam or not, tumor is benign or malignant, etc. are binary classification as it deals with two categories in the target class. When there are more than two categories in the target class, the classification problem resides to multilabel classification and example might be like classifying cars company based on image whether it is Honda or Volkswagen or Renault.</p> <p>The classification algorithm k-Nearest Neighbors classifier and Naïve Bayes classifier are two classifiers that better suits the classification problem. The performance metrics like Confusion matrix, Accuracy, F1 score, Precision, Recall, Heatmap, etc. gives the insight of model performance.</p> <h2 id="ii-algorithms">II. Algorithms</h2> <p>The convention used in the derivation includes a collection of labeled examples \(\{(x*i,yi)\}*{i=1}^N\) , where N is the size of the collection, \(x_i\) is the D-dimensional feature vector of example \(i=1, 2, …, N\) , \(y_i\) is a real-valued target and every feature \(x_i^{(j)}\) , \(j=1, 2, …, D\) , is also a real number.</p> <h3 id="a-k-nearest-neighbors-classifier">A. k-Nearest Neighbors Classifier</h3> <p>k-Nearest Neighbors (kNN) is non parametric and instance-based learning algorithm. Contrary to other learning algorithms, it keeps all training data in memory. Once new, previously unseen example comes in, the kNN algorithm finds k training examples closest to x and returns the majority label.</p> <p>The closeness of two examples is given by a distance function. For example, Euclidean distance is frequently used in practice. Euclidean distance between \(x_i\) and \(x_k\) is given as,</p> \[d(\boldsymbol {x_i, x_k}) = \sqrt{(x_i^{(1)}-x_k^{(1)})^2 + (x_i^{(2)}-x_k^{(2)})^2 + ... + (x_i^{(N)}-x_k^{(N)})^2} \tag1\] <p>The Euclidean distance in summation of the vector is given as;</p> \[d(\boldsymbol {x_i, x_k}) = \sqrt{\sum_{j=1}^{D}(x_i^{(j)}-x_k^{(j)})^2} \tag2\] <p>Another popular choice of the distance function is the negative cosine similarity. Cosine similarity defined as,</p> \[s(\boldsymbol {x_i, x_k})=\frac{\sum_{j=1}^{D}x_i^{(j)}x_k^{(j)}}{\sqrt{\sum_{j=1}^{D}(x_i^{(j)})^2}\sqrt{\sum_{j=1}^{D}(x_k^{(j)})^2}} \tag3\] <p>The Equation (3) gives a measure of similarity of the directions of the two vectors and \(𝑠(\boldsymbol {x_i, x_k})\) can also be denoted as \(cos(\boldsymbol{x_i, x_k})\) ). If the angle between two vectors is \(0\) degrees, then two vectors point to the same direction, and cosine similarity is equal to \(1\). If the vectors are orthogonal, the cosine similarity is \(0\). For vectors pointing in opposite directions, the cosine similarity is \(−1\). If we want to use cosine similarity as a distance metric, we need to multiply it by \(−1\). Other popular distance metrics include Minkowski distance, Chebychev distance, Mahalanobis distance, and Hamming distance. The choice of the distance metric, as well as the value for k, are the choices the analyst makes before running the algorithm. The k-NN classifier starts with loading the data into memory. The value of k (number of neighbors) defines the prediction boundaries that means how much sorted distances are taken into account to find the mode of the k labels. The algorithm takes votes to classify the labels among selected k-neighbors. It returns the majority class labels leaving behind minority. The flowchart of the k-NN classifier is;</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/datamining-KNN-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/datamining-KNN-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/datamining-KNN-1400.webp"/> <img src="/assets/img/datamining-KNN.png" width="auto" height="auto" alt="datamining-knn" style="height: 50rem;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Fig. 1.1: k-NN flowchart</figcaption> </figure> <p>The selection of the hyperparameter k has a significant effect on the classifier. In general, for the lower value of k, the classifier may overfit on new unseen data. The value of k is chosen such that balances bias and variance. When k is small, we are restraining the region of a given prediction and forcing our classifier to be “more blind” to the overall distribution. A small value for K provides the most flexible fit, which will have low bias but high variance. Graphically, our decision boundary will be more jagged. On the other hand, a higher k averages more voters in each prediction and hence is more resilient to outliers. Larger values of k will have smoother decision boundaries which means lower variance but increased bias. The value of k is chosen such that the desired accuracy of kNN classifier is achieved. The simple method to calculate the value of k is plotting error versus k graph and choosing the k on which error is minimum.</p> <h3 id="b-naïve-bayes-classifier">B. Naïve Bayes Classifier</h3> <p>Bayes’ Rule or Bayes’ Theorem is a statistical principle for combining prior knowledge of the classes with new evidence gathered from data. The class-conditional probability 𝑃(𝑋|𝑌), and the evidence, P(X):The Bayes’ Rule (also known as the Bayes’ Theorem) stipulates that:</p> \[P(𝑌|\boldsymbol{X}) = \frac{P(\boldsymbol{X}|𝑌) P(𝑌)} {P(\boldsymbol{X})} \tag4\] <p>In Bayes’ rule (4), it finds the probability of event 𝑌, given that the event 𝑋 is true. Event 𝑋 is also termed as evidence. 𝑃(𝑌) is the priori of 𝑌 (the prior probability, i.e. Probability of event before evidence is seen). 𝑃(𝑌|𝑿) is a posteriori probability of 𝑋, i.e. probability of event after evidence is seen. A Naïve Bayes classifier estimates the class-conditional probability by assuming that the attributes are conditionally independent, given the class label 𝑦. Here, 𝑃(𝑿) is a class probability and 𝑃(𝑿|𝑦) is a conditional probability. The conditional independence assumption can be formally stated as follows:</p> \[𝑃(\boldsymbol{X}|𝑌 = 𝑦) = \prod_{i=1}^d𝑃(𝑋_𝑖|𝑌 = 𝑦)\tag5\] <p>Where each attribute set \(\boldsymbol{X} = \{𝑋*1 ,𝑋_2, … ,𝑋*𝑑\}\) consists of d attributes. The Naïve Bayes is also called Simple Bayes as it assumes that features of a measurement are independent of each other and makes equal contribution to the outcome.</p> <h2 id="iii-metrics">III. METRICS</h2> <p>The classifier model doesn’t always give the accurate result. There are some parameters to measure how the classifier behave with unseen data to classify like Confusion matrix, Accuracy, F1 score, Precision, Recall, Heatmap etc. The different evaluation metrics are used for different kinds of problems. We build a model, get feedback from metrics, make improvements and continue until we achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspect of evaluation metrics is their capability to discriminate among model results.</p> <h3 id="a-confusion-matrix">A. Confusion Matrix</h3> <p>The confusion matrix is a table that summarizes how successful the classification model is at predicting examples belonging to various classes. One axis of the confusion matrix is the label that the model predicted, and the other axis is the actual label. In a binary classification problem, there are two classes. Let’s say, the model predicts two classes: “spam” and “not_spam”:</p> \[\begin{array} {|r|r|}\hline &amp; &amp; spam (predicted) &amp; not_spam(predicted) \\ \hline &amp; spam (actual)&amp; 23 (TP) &amp; 1 (FN) \\ \hline &amp; not_spam (actual) &amp; 12 (FP) &amp; 556(TN) \\ \hline \end{array}\] <p>The above confusion matrix shows that of the 24 examples that actually were spam, the model correctly classified 23 as spam. In this case, we say that we have 23 true positives or TP = 23. The model incorrectly classified 1 example as not_spam. In this case, we have 1 false negative, or FN = 1. Similarly, of 568 examples that actually were not spam, 556 were correctly classified (556 true negatives or TN = 556), and 12 were incorrectly classified (12 false positives, FP = 12).</p> <h3 id="b-precisionrecall">B. Precision/Recall</h3> <p>The two most frequently used metrics to assess the model are precision and recall. Precision is the ratio of correct positive predictions to the overall number of positive predictions:</p> \[precision = \frac{𝑇𝑃}{𝑇𝑃 + 𝐹𝑃}\tag6\] <p>Recall is the ratio of correct predictions to the overall number of positive examples in the datasets:</p> \[recall = \frac{𝑇𝑃}{TP+FN} \tag7\] <p>In the case of the spam detection problem, we want to have high precision (we want to avoid making mistakes by detecting that a legitimate message is spam) and we are ready to tolerate lower recall (we tolerate some spam messages in our inbox). The goal of classifier model is to choose between a high precision or a high recall. It’s usually impossible to have both. The hyperparameter tuning helps to maximize precision or recall.</p> <h3 id="c-accuracy">C. Accuracy</h3> <p>Accuracy is given by the number of correctly classified examples divided by the total number of classified examples. In terms of the confusion matrix, it is given by:</p> \[accuracy = \frac{TP+TN}{TP+TN+FP+FN} \tag8\] <p>Accuracy is a useful metric when errors in predicting all classes are equally important.</p> <h3 id="d-f1-score">D. F1 Score</h3> <p>F1-Score is the harmonic mean of precision and recall values for a classification problem. The formula for F1-Score is as follows:</p> \[F1 = \frac{recall^{-1}+precision^{-1}}{2} \tag9\] \[F1 = 2.\frac{precision.recall}{precision + recall} \tag{10}\] <p>The general formula for positive real β, where β is chosen such that recall is considered β times as important as precision, is:</p> \[F_{\beta}=(1+{\beta}^2) \cdot \frac{precision \cdot recall}{({\beta}^2 \cdot precision)+recall} \tag{11}\] <p>The equation (11) or \(𝐹_𝛽\) measures the effectiveness of a model with respect to a user who attaches β times as much importance to recall as precision.</p> <h3 id="e-heat-map">E. Heat Map</h3> <p>The heat map can be elucidated as a cross table or spreadsheet which contains colors instead of numbers. The default color gradient sets the lowest value in the heat map to dark blue, the highest value to a bright red, and mid-range values to light gray, with a corresponding transition (or gradient) between these extremes. Heat maps are well-suited for visualizing large amounts of multi-dimensional data and can be used to identify clusters of rows with similar values, as these are displayed as areas of similar color.</p> <h2 id="iv-result">IV. RESULT</h2> <p>The value of hyperparameter like k in the k-NN classifier plays a significant role to correctly classify the labels or target variables. The error versus k values plot provides a guideline to choose k and the value of k with minimum error is chosen.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/k_value_vs_error-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/k_value_vs_error-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/k_value_vs_error-1400.webp"/> <img src="/assets/img/k_value_vs_error.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Fig. 5.1: Error versus K-value</figcaption> </figure> <p>Fig. 5. 1 shows the fluctuation of error at different values of k and the graph is not continuous. We would rather prefer to calculate minimum error k-value than maximum error k-value as minimum error k-value gives more accurate prediction. The minimum error of k-NN classifier model for test set is at 𝑘 = 12 and the error is 0.0467 (i.e. 4.67%). Hence, 𝑘 = 12 is chosen as k-value for k-NN classifier. The performance metrics of kNN classifier with parameters metric as ‘minkowski’, neighbors as ‘12’are:</p> <p>Confusion matrix: [[136 6] <br/> $\qquad$ $\qquad$ $\qquad$ [8 150]],<br/> Precision for label ‘0’ prediction: 0.94, <br/> Precision for label ‘1’ prediction: 0.96, <br/> Recall for label ‘0’ prediction: 0.96, <br/> Recall for label ‘1’ prediction: 0.95, <br/> F1-score for label ‘0’ prediction: 142, <br/> F1-score for label ‘1’ prediction: 158, <br/> Accuracy: 0.95 <br/> The model has classified the labels with 𝑇𝑃 = 136,𝐹𝑁 = 6, 𝐹𝑃 = 8, 𝑇𝑁 = 150 that means model misclassified 6 labels as label ‘1’ which is actually label ‘0’ and misclassified 8 labels as label ‘0’ which is actually label ‘1’. Hence, the model has an accuracy of about 95%.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/heat_map-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/heat_map-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/heat_map-1400.webp"/> <img src="/assets/img/heat_map.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Fig. 5.2: Heat map predicted label over the true label</figcaption> </figure> <p>Fig. 5. 2 Heat map predicted label over the true label Heat map is a graphical representation of value in the confusion matrix obtained from the predicted label and actual target name. In the above heatmap, the red square denotes the maximum value on the confusion matrix and with a decrease in value the color fades up. Diagonal elements have a higher value as shown in the heatmap which shows a higher performance of the classification model and informs predicated label matches the true label for any given input.</p> <p>For the given model, “prime minister of nepal” supplied as input assign a label “talk.politics.mideast” similarly , when “joker” is supplied as input assign a label “comp.sys.ibm.pc.hardware”. Here for two different input two different label has been assigned out of which one label assigned for the input “prime minister of nepal” is correct whereas for “joker” correct label has not been assigned properly which is due to naïve base treating the input as independent values as well as lack of data being supplied.</p> <h2 id="conclusion">Conclusion</h2> <p>The two popular classifiers k-NN and Naïve Bayes provide good accuracy to the model. Many parameters contribute to model performance. The right choice of hyperparameter also yields a better result. There is no rule of thumb to select the right value of hyperparameter for the first trial and the hyperparameter value that works fine for one model may not yield the same result for another model. The good model is that which considers all the performance metric parameters like Accuracy, F1-score, Precision, Recall, etc. Though we have so many metrics parameters to evaluate the model performance, some analytics is needed to better explain the metric that addresses classification problems in the best possible way. The contribution of all performance metrics needs to be analyzed to make the model accurate.</p> <h2 id="appendix">Appendix</h2> <h3 id="a-k-nn-classifier">A. k-NN classifier</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Importing the libraries
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span>
<span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Importing the dataset
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">datasets/Dataset_1.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Quick look at data
</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Standardizing the variables
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">TARGET CLASS</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">scaled_features</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">TARGET
CLASS</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">df_feat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">scaled_features</span><span class="p">,</span>
<span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># After standardization
</span><span class="nf">print</span><span class="p">(</span><span class="n">df_feat</span><span class="p">.</span><span class="nf">head</span><span class="p">())</span>

<span class="c1"># train test split
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span>
 <span class="n">scaled_features</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">TARGET CLASS</span><span class="sh">'</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
<span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initializing error and k_value list
</span><span class="n">error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">k_value</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">k_value</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Using KNN
</span>    <span class="n">knn</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">knn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">error_</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">error</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">error_</span><span class="p">)</span>

<span class="c1"># Plotting k_value and error
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_value</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">K value</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Error</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">k_value_vs_erro.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="n">bbox_inches</span><span class="o">=</span><span class="sh">"</span><span class="s">tight</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">performance_report</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span>
<span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># k-NN classifier
</span>    <span class="n">knn</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">knn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">confusion_matrix_</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">classification_report_</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">confusion_matrix</span><span class="sh">'</span><span class="p">:</span> <span class="n">confusion_matrix_</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">classification_report</span><span class="sh">'</span><span class="p">:</span> <span class="n">classification_report_</span>
 <span class="p">}</span>

<span class="c1"># to numpy
</span><span class="n">error_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
<span class="n">k_value_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">k_value</span><span class="p">)</span>
<span class="n">error_min_index</span> <span class="o">=</span> <span class="n">error_np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span> <span class="c1"># numpy int to
</span><span class="n">python</span> <span class="nb">int</span>
<span class="n">k_value_</span> <span class="o">=</span> <span class="n">k_value_np</span><span class="p">[</span><span class="n">error_min_index</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">K= {} and error= {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">k_value_</span><span class="p">,</span>
<span class="n">error_np</span><span class="p">[</span><span class="n">k_value_</span><span class="p">]))</span>

<span class="c1"># for minimum error
</span><span class="n">performance_report_</span> <span class="o">=</span> <span class="nf">performance_report</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">k_value_</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">For k = {}: </span><span class="se">\n</span><span class="s"> {}{}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">k_value_</span><span class="p">,</span>
<span class="n">performance_report_</span><span class="p">[</span><span class="sh">'</span><span class="s">confusion_matrix</span><span class="sh">'</span><span class="p">],</span>
<span class="n">performance_report_</span><span class="p">[</span><span class="sh">'</span><span class="s">classification_report</span><span class="sh">'</span><span class="p">]))</span>
</code></pre></div></div> <h3 id="b-naïve-bayes-classifier-1">B. Naïve Bayes classifier</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Importing the libraries
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">datasets</span> <span class="n">impor</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="n">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1"># importing the dataset
</span><span class="n">data</span> <span class="o">=</span> <span class="nf">fetch_20newsgroups</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>

<span class="c1"># training the data on these categories
</span><span class="n">categories</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">target_names</span>
<span class="n">train</span> <span class="o">=</span> <span class="nf">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">,</span>
<span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="nf">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">,</span>
<span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">train</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># Pipelining the model
</span><span class="n">model</span> <span class="o">=</span> <span class="nf">make_pipeline</span><span class="p">(</span><span class="nc">TfidfVectorizer</span><span class="p">(),</span>
<span class="nc">MultinomialNB</span><span class="p">())</span>

<span class="c1"># Fitting the data
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">train</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">test</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># heatmap
</span><span class="n">mat</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">test</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">,</span>
<span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">train</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">true label</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">predicted label</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># plt.tight_layout()
</span><span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">images/lab04/heat_map.jpg</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="n">bbox_inches</span><span class="o">=</span><span class="sh">"</span><span class="s">tight</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># predicting
</span><span class="k">def</span> <span class="nf">predict_category</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">train</span><span class="p">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

<span class="c1"># prediction
</span><span class="nf">print</span><span class="p">(</span><span class="nf">predict_category</span><span class="p">(</span><span class="sh">'</span><span class="s">Jesus christ</span><span class="sh">'</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">predict_category</span><span class="p">(</span><span class="sh">'</span><span class="s">Prime minister of Nepal</span><span class="sh">'</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">predict_category</span><span class="p">(</span><span class="sh">'</span><span class="s">Everest</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <h2 id="references">References</h2> <ul> <li>P. Tan, M. Steinbach, V. Kumar and A. Karpatne, Introduction to Data Mining, Global Edition. Harlow, United Kingdom: Pearson Education Limited, 2019.</li> <li>A. Burkov, The hundred-page machine learning book, Global Edition. Quebec City, Canada, 2019.</li> </ul>]]></content><author><name>Sudip Bhujel</name></author><category term="journal"/><category term="datamining"/><category term="datascience"/><category term="machinelearning"/><summary type="html"><![CDATA[I. Introduction]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sudipbhujel.github.io/knnandnaivebayes.png"/><media:content medium="image" url="https://sudipbhujel.github.io/knnandnaivebayes.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Data Mining Parameters</title><link href="https://sudipbhujel.github.io/blog/2019/data-mining-parameters/" rel="alternate" type="text/html" title="Data Mining Parameters"/><published>2019-12-28T00:00:00+00:00</published><updated>2019-12-28T00:00:00+00:00</updated><id>https://sudipbhujel.github.io/blog/2019/data-mining-parameters</id><content type="html" xml:base="https://sudipbhujel.github.io/blog/2019/data-mining-parameters/"><![CDATA[<p>Datamining covers everything that are related with the data from collection of raw data to EDA and preparation of input to AI algorithm. We have lots of parameters for describing the data. Some of them we are going to discuss are Impurity index, Central of tendency, Eigenvalue/ Eigenvector, PCA in Classification. <br/> <strong> Abstract: <i> The impurities measurement parameter of dataset like Entropy, Gini, Classification Error aims to find the error while classifying the labels. The attribute with less value of impurity will be chosen out of attribute contenders. The measure of central tendency like mean, median, quartiles, etc. along with boxplot gives the idea about the distribution of data and outliers which leads then how to treat the data to get the most information out of it. The features/ attributes are important parameters for any machine learning algorithm, large-sized attributes result in a more accurate prediction which means that the model has high accuracy. The computational cost for a model with a large number of attributes is generally high. The best model is that which takes as least attributes as possible without losing the information and has reasonable accuracy. Principal Component Analysis (PCA) is a feature extraction method that uses orthogonal linear projections to capture the underlying variance of the data. It reduces the number of least wanted features for prediction without losing the overall information of data. </i></strong></p> <h2 id="entropy">Entropy</h2> <p>Entropy is a measure of impurity, disorder or uncertainty in a bunch of examples i.e. it is an indicator of how messy our data is. In Decision Trees, the goal is to tidy the data. Entropy controls how a Decision Tree decides to split the data. It affects how a Decision Tree draws its boundaries so that the outcomes from the algorithm will have purely classified objects.</p> \[E(x) = \sum_{x\epsilon X} p(x)log_2 p(x)\] <p>Where, S = The current dataset for which entropy is being calculated <br/> X = Set of classes in S<br/> p(x) = The probability of each set S</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/impurity-vs-probability-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/impurity-vs-probability-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/impurity-vs-probability-1400.webp"/> <img src="/assets/img/impurity-vs-probability.png" class="img-fluid z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="color:grey; text-align:center; font-style:italic"> Impurity Index versus Probability, Impurity Indices are Entropy, Gini, and Classification Error</p> <h2 id="gini">Gini</h2> <p>Impurity measures such as entropy and Gini index tend to favor attributes that have a large number of distinct values . If we consider the same example as in entropy, the gini index is computed using the following equation:</p> \[G(S) = 1-\sum_{x\epsilon X} |p(x)|^2\] <p>Where, S = The current dataset for which entropy is being calculated <br/> X = Set of classes in S <br/> p(x) = The probability of each set S</p> <h2 id="classification-error">Classification Error</h2> <p>Classification error is a measure of impurity at a node and defined for classification error at a node t as,</p> \[Error(t) = 1 − maxP(i|t)\] <p>The classification error made by node ranges minimum 0 when all records belong to one class to maximum \((1 − 1/n_c )\) when records are equally distributed among all classes.</p> <h2 id="covariance-matrix">Covariance Matrix</h2> <p>Variance measures the variation of a single random variable (like the height of a person in a population), whereas covariance is a measure of how much two random variables vary together (like the height of a person and the weight of a person in a population). The covariance matrix can be calculated using covariance, which is a square matrix given by C I,j = σ(x i , x j ) where C ∈ R d xd and d describe dimension or number of random variables of the data (e.g. the number of features like height, width, weight, etc.). The calculation for the covariance matrix can be also expressed as:</p> \[C = \frac{1}{n-1} \sum_{i=1} ^n (X_i-\overline{X} )(X_i-\overline{X} )^T\] <p>The covariance matrix for two dimensions is given by,</p> \[\begin{pmatrix} \sigma(x,x) &amp; \sigma(x,y) \\\ \sigma(y,x) &amp; \sigma(y,y) \end{pmatrix}\] <p>The covariance matrix is symmetric since \(\sigma(x_i, x_j) = \sigma(x_j, x_i)\).</p> <h2 id="eigenvalue-and-eigenvector">Eigenvalue and Eigenvector</h2> <p>In linear algebra, an eigenvector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue is the factor by which the eigenvector is scaled. For linear equations:</p> \[Av = λv\] <p>In this equation A is an n-by-n matrix, v is a non-zero n-by-1 vector and \(\lambda\) is a scalar (which may be either real or complex). Any value of \(\lambda\) for which this equation has a solution is known as eigenvalue of the matrix A. It is sometimes also called the characteristics value. The vector, v, which corresponds to this value is called an eigenvector. The eigen problem can be written as</p> \[A. v − \lambda. v = 0 \\ A. v − \lambda. I. v = 0 \\ (A − \lambda. I). v = 0\] <p>If v is non-zero, this equation will only have a solution if \(|A − \lambda. I| = 0\) This equation is called the characteristic equation of A, and is an nth order polynomial in \(\lambda\) with n roots. These roots are called the eigenvalues of A. We will only deal with the case of n distinct roots, though they may be repeated. For each eigenvalue, there will be an eigenvector for which the eigenvalue equation is true.</p> <h2 id="distances">Distances</h2> <p><strong>Euclidean distance</strong> is a measure of the distance between two points in Euclidean space. Mathematically,</p> \[dist = \sqrt{\sum_{k=1}^n (p_k - q_k)^2}\] <p>Where n is the number of dimensions (attributes) and \(p_k\) and \(q_k\) are, respectively, the \(k^th\) attributes (components) or data objects p and q. <strong>Minkowski Distance </strong> is a generalization of Euclidean distance and given as,</p> \[dist = \left(\sum_{k=1}^n |p_k - q_k|^r \right)^{\frac{1}{r}}\] <p>Where r is a parameter, n is the number of dimensions (attributes) and \(p_k\) and \(q_k\) are, respectively, the k th attributes (components) or data objects p and q.</p> <ul> <li>r = 1, it becomes Manhattan distance.</li> <li>r = 2, it becomes Euclidean distance.</li> <li>\(r \to \infty\), it becomes supremum distance.</li> </ul> <h2 id="similarity">Similarity</h2> <p>The similarity is the measure of how much alike two data objects are. The similarity in a data mining context is usually described as a distance with dimensions representing features of the objects. A small distance indicating a high degree of similarity and a large distance indicating a low degree of similarity. The similarity is subjective and is highly dependent on the domain and application.<br/> <strong>Cosine Similarity</strong> of two document vectors is given as,</p> \[cos(d_1, d_2) = \frac{d_1 . d_2}{||d_1||.||d_2||}\] <p>Where ||d|| is the length of vector d.<br/> Cosine similarity is for comparing two real-valued vectors, but <strong>Jaccard similarity</strong> is for comparing two binary vectors (sets). Mathematically,</p> \[J_g (a,b) = frac{sum_i min(a_i, b_i)}{sum_i max(a_i, b_i)}\] <p>For example, \(t_1 = (1, 1,0,1), t_2 = (2,0,1,1)\), the generalized Jaccard similarity index can be computed as follows:</p> \[J(t_1, t_2) = \frac{1+0+0+1}{2+1+1+1} = 0.4\] <h2 id="pca">PCA</h2> <p>Principal Component Analysis (PCA) is a feature extraction method that uses orthogonal linear projections to capture the underlying variance of the data. The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. It reduces the dimension of the data with the aim of retaining as much information as possible. In other words, this method combines highly correlated variables to form a smaller number of an artificial set of variables which is called “principal components” that account for the most variance in the data.</p> <h2 id="conclusion">CONCLUSION</h2> <p>The measure of central of tendency, similarity, etc. are the part of Exploratory Data Analysis (EDA). The EDA itself doesn’t give the model for prediction but extremely useful for getting the sense of information from data. This gives an idea about how to get started with the data. Impurity indices like Entropy, Gini, and Classification Error in the classification helps examine how classification algorithm struggles to classify the items based on their attributes. The impurity index helps find the depth of the decision tree algorithm.</p> <h2 id="references">References</h2> <ul> <li><a href="https://www.amazon.com/Introduction-Mining-Whats-Computer-Science/dp/0133128903/ref=sr_1_5?keywords=data+mining&amp;qid=1577535801&amp;sr=8-5">P. Tan, M. Steinbach, V. Kumar and A. Karpatne, Introduction to Data Mining, Global Edition. Harlow, United Kingdom: Pearson Education Limited, 2019.</a></li> </ul>]]></content><author><name>Sudip Bhujel</name></author><category term="journal"/><category term="datamining"/><category term="EDA"/><summary type="html"><![CDATA[Datamining covers everything that are related with the data from collection of raw data to EDA and preparation of input to AI algorithm. We have lots of parameters for describing the data. Some of them we are going to discuss are Impurity index, Central of tendency, Eigenvalue/ Eigenvector, PCA in Classification. Abstract: The impurities measurement parameter of dataset like Entropy, Gini, Classification Error aims to find the error while classifying the labels. The attribute with less value of impurity will be chosen out of attribute contenders. The measure of central tendency like mean, median, quartiles, etc. along with boxplot gives the idea about the distribution of data and outliers which leads then how to treat the data to get the most information out of it. The features/ attributes are important parameters for any machine learning algorithm, large-sized attributes result in a more accurate prediction which means that the model has high accuracy. The computational cost for a model with a large number of attributes is generally high. The best model is that which takes as least attributes as possible without losing the information and has reasonable accuracy. Principal Component Analysis (PCA) is a feature extraction method that uses orthogonal linear projections to capture the underlying variance of the data. It reduces the number of least wanted features for prediction without losing the overall information of data.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sudipbhujel.github.io/data-mining-parameters.jpeg"/><media:content medium="image" url="https://sudipbhujel.github.io/data-mining-parameters.jpeg" xmlns:media="http://search.yahoo.com/mrss/"/></entry></feed>